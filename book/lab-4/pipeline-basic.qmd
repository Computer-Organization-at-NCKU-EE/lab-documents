# Fundamental and Motivation of Pipelining {#sec-pipeline-basics}

## The Motivation

### What is ***disadvantages*** of the single-cycle design?

- Critical Path is too long
- Low maximum clock frequency 
- Not compatible with non-ideal memory where the access latency is not zero

The overriding motivation for pipelining is to raise the maximum clock frequency: because the clock period is ultimately set by the [**critical path period**]{.mark}, breaking the work into stages shortens that delay per stage and lets the design run faster.
 
## The Fundamental

### How to shorten the critical path?
> What is the essence of *pipeline* design?

- Visualization of pipeline design

  Picture a CPU as a water pipe: single-cycle is one long pipe where the slowest section sets the pace, while pipelining slices it with “tanks” so packets flow concurrently—shortening per-stage delay and raising the maximum clock frequency.

![Visualization of pipeline design](images/pipeline.png){width=80% fig-align="center"}




- The micro-architecture of a classic five stages pipeline CPU
  
  A classic 5-stage pipeline (IF, ID, EX, MEM, WB) is spread across clock cycles, and each stage takes exactly one clock. An instruction needs 5 cycles of latency to pass all stages, but once the pipe is full, a new instruction completes every clock. 

  ![This figure is from @baer2009microprocessor](images/pipeline_architecture.png){width=80% fig-align="center"}


  Focusing on the datapath: before pipelining, a `LOAD` goes through 10+6+6+10+3 = 35 units of delay, so the overall critical path is 35. With one instruction completed per clock, the clock period must be ≥ 35. 
  
  With pipelining, we choose the clock period to match the slowest stage—in this case, MEM. That makes the per-stage critical path 10, so the design can run with a 10-unit clock period, ignoring pipeline-register overhead.

  ![abstracted pipeline](images/abstracted_pipeline_time.png){width=70% fig-align="center"}


### What is the problem about pipeline CPU?

::: {.callout-warning}
What’s the catch with a pipelined CPU? Hazards introduce pipeline bubbles, which lower the instructions-per-cycle (IPC). And even with pipelining, if the core is scalar rather than superscalar, *the ideal maximum IPC is still 1—this point is crucial*.
:::

- Structural Hazard (resource contention)
  
  Problem : Hardware resources are not enough.

  Problem in Pipeline CPU : Accessing memory at the same time by fetching instructions and loading data.

  Solution : We duplicate SRAM as im & dm to solve memory access problem of simultaneous instruction fetch and load data.

- Control Hazard
  
  Problem : There is an indeterminate instruction flow in the pipeline.

  Problem in Pipeline CPU : The subsequent instructions have entered the pipeline before the jump or branch is determined.
  
  Solution : We implement a flush signal in the Controller to flush the wrong instructions in pipeline registers.

  - Branch and Jump Instructions with pipeline CPU
    
  - Branch Prediction

- Data Hazard
  Problem : Unable to get the latest data for calculations.
  
  Problem in Pipeline CPU : If the result data of an instruction needs to be writeback, subsequent instructions cannot get it until it completes.The root cause is [**the limited number of architectural registers**]{.mark}.
  
  Solution : We forwards the writeback data from MEM stage & WB stage to ID stage & EX stage

  - 4 types of data hazards
    1. Read-After-Write (true dependency)
    2. Write-After-Read (anti-dependency)
    3. Write-After-Write (output dependency)
    4. Read-After-Read (false dependency)//這個有嗎
   
  ::: {.callout-note}
  In essence, both false dependencies and anti-dependencies stem from the ISA’s limited set of architectural registers. By providing more physical registers and using register renaming, we can eliminate these constraints and enable out-of-order execution.
  :::
  
- Load-Use Hazard (Special Case)
  
  Fundamentally, it’s about memory latency: if you force the pipeline to wait for each memory request to complete, you lengthen the critical path and drag down overall performance—so even with a higher IPC, the machine can end up slower. 
  
  This is a good reminder that in quantitative analysis we must avoid the pitfall of optimizing for a single metric.





